All the sources that I found usefull or interesting. Can be documents, blogs, librairies or courses that are either in french or in english.

Machine Learning:
http://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html (loss functions)
http://ruder.io/optimizing-gradient-descent/ (gradient descent)
https://stats.stackexchange.com/questions/222081/which-algorithm-can-learn-exactly-a-tree-structure-without-noise/222125#222125
https://www.casact.org/newsletter/pdfupload/ar/brainstorms_corrected_from_ar_may2009.pdf (skewed distribution)

Interpretability
https://arxiv.org/pdf/1602.04938.pdf

Linear Regression
https://hec.unil.ch/docs/files/42/233/statistique_4_2.pdf
https://assessingpsyche.wordpress.com/2014/07/10/two-visualizations-for-explaining-variance-explained/ (variance expliquée)
https://stats.stackexchange.com/questions/202221/for-linear-classifiers-do-larger-coefficients-imply-more-important-features/202846 (importance var)
https://robjhyndman.com/hyndsight/intervals/ (intervalle de confiance)
http://people.duke.edu/~rnau/regintro.htm

XGBoost
https://www.slideshare.net/ShangxuanZhang/kaggle-winning-solution-xgboost-algorithm-let-us-learn-from-its-author
https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d (Gradient Boosting)

RandomForest
https://www.quora.com/Why-does-random-forest-use-sampling-with-replacement-instead-of-without-replacement (bagging)
http://www.utdallas.edu/~nrr150130/cs7301/2016fa/lects/Lecture_10_Ensemble.pdf (reduction de variance par methodes ensembliste)

Time series
https://www.math.univ-toulouse.fr/~lagnoux/Poly_SC.pdf
https://eric.univ-lyon2.fr/jcugliari/pdf/series-slides.pdf
https://www.otexts.org/fpp/2/5 (Eval Serie temp)
https://tomholderness.wordpress.com/2013/01/10/confidence_intervals/ (Intervalle de confiance)
https://gist.github.com/riccardoscalco/5356167 (Intervalle de confiance)
https://www.r-bloggers.com/better-prediction-intervals-for-time-series-forecasts/ (Intervalle de confiance)
https://otexts.org/fpp2/intro.html (complet)

Metrics
Binary classification
http://ftp.cs.wisc.edu/machine-learning/shavlik-group/davis.icml06.pdf https://sanchom.wordpress.com/tag/average-precision/

Neural nets
https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly https://playground.tensorflow.org/ https://arxiv.org/abs/1502.03167
https://mapr.com/blog/deep-learning-tensorflow/
http://cs231n.github.io/neural-networks-1/ (remplacer par 2 et par 3 aussi)
https://rolisz.ro/2013/04/18/neural-networks-in-python/ (réseau de neurones implémenté en Python)
http://mmlab.ie.cuhk.edu.hk/ (reconnaissance faciale)
https://stanford.edu/~shervine/l/fr/teaching/cs-230/ (TOUT EN FRANCAIS)
https://github.com/facebookresearch/maskrcnn-benchmark (maskRCNN)
https://arxiv.org/pdf/1606.07792.pdf (Wild And Deep) https://arxiv.org/ftp/arxiv/papers/1803/1803.00202.pdf (recommandation) https://arxiv.org/abs/1605.07110 (deepearning without local minima)
https://arxiv.org/pdf/1706.02515.pdf (batch-norm)
https://arxiv.org/pdf/1902.06838.pdf (gans)
https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b (vanishing gradient)

CNN
https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html#r-cnn (rcnn fastRcnn/faster)
http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf (rcnn fastRcnn/faster)
http://www.huppelen.nl/publications/selectiveSearchDraft.pdf (selective search)
https://jhui.github.io/2017/03/15/Fast-R-CNN-and-Faster-R-CNN/ (rcnn fastRcnn/faster)
https://www.youtube.com/watch?v=4tkgOzQ9yyo (MaskRCNN)
https://arxiv.org/pdf/1901.09321.pdf (pour une bonne initialisation)
https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf (Comprendre les spécialisations des neurones)
https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf (transfers de style)
https://arxiv.org/pdf/1712.05884.pdf https://ml4a.github.io/ml4a/convnets/

Image similarity

https://github.com/karpathy/paper-notes/blob/master/matching_networks.md https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1142.pdf http://places.csail.mit.edu/places_NIPS14.pdf https://www.quora.com/What-are-the-advantages-of-using-a-triplet-loss-function-over-a-contrastive-loss-How-would-you-decide-which-to-use https://arxiv.org/ftp/arxiv/papers/1709/1709.08761.pdf

RNN :
http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L14%20Recurrent%20Neural%20Nets.pdf
http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf

LSTM :
http://philipperemy.github.io/keras-stateful-lstm/ (statefull)
https://www.quora.com/Im-having-a-hard-time-conceptualizing-the-difference-between-stateful-and-stateless-LSTMs-in-Keras-What-does-one-choose-stateful-vs-stateless (statefull)
http://karpathy.github.io/2015/05/21/rnn-effectiveness/
https://www.quora.com/In-LSTM-how-do-you-figure-out-what-size-the-weights-are-supposed-to-be
https://datascience.stackexchange.com/questions/19196/forget-layer-in-a-recurrent-neural-network-rnn/19269#19269
https://datascience.stackexchange.com/questions/20413/clarification-on-the-keras-recurrent-unit-cell?noredirect=1&lq=1
https://www.quora.com/What-is-the-meaning-of-%E2%80%9CThe-number-of-units-in-the-LSTM-cell
https://blog.openai.com/unsupervised-sentiment-neuron/
http://konstilackner.github.io/LSTM-RNN-Melody-Composer-Website/
https://medium.com/@xenonstack/overview-of-artificial-neural-networks-and-its-applications-2525c1addff7
https://sflscientific.com/data-science-blog/2017/2/10/predicting-stock-volume-with-lstm
https://eng.uber.com/neural-networks/
https://eng.uber.com/neural-networks-uncertainty-estimation/ (intervalle de confiance)
https://mapr.com/blog/deep-learning-tensorflow/
http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction
keras-team/keras#6168 (exemple de prog fonctionnelle)
https://www.quora.com/When-should-I-use-an-RNN-LSTM-and-when-to-use-ARIMA-for-a-time-series-forecasting-problem-What-is-the-relation-between-them (comparaison ARIMA)

NLP:
https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50

MLOps:
https://storage.googleapis.com/pub-tools-public-publication-data/pdf/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf

Cool Libraries:
https://github.com/mdbloice/Augmentor
https://github.com/awslabs/deequ
https://github.com/pallets/click
https://github.com/awslabs/aws-data-wrangler
https://github.com/nalepae/pandarallel
https://github.com/tslearn-team/tslearn

Miscellaneous
Steam inputs keras : keras-team/keras#8130 & https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly

Dev
https://nvie.com/posts/a-successful-git-branching-model/
